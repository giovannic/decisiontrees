Implentation

Cross validation

We decided to use fold sizes of floor(|examples|/folds), which in this case was floor(1004/10) = 100, to get the largest most evenly sized folds.

The data was then iterated over 10 times producing 6 sets of trees trained with 904 examples to predict 100 results per set. No predictions are made for the last 4 examples.

The decision trees are trained by rooting the attribute which will result in the greatest information gain so we can reduce the size of our tree. We calculate the information gain by first calculating the entropy of the current set of examples. For each attribute we then calculate the entropies for the examples produced for each possible outcome, 0 or 1, and weight them on the fraction of the examples they will produce. These entropies are then subtracted from the total gain to give the gain. The attribute with the largest gain is selected.

We considered setting a threshold on the information gain required to continue down a branch. This would sorten the tree and reduce overfitting. However we went for a reduced error pruning approach.

[reduced error pruning]

We make our predictions by walking the 6 trees, ignoring results with no successes across the trees and resolving conflicts in various ways: randomly, on common outcomes and on the depth of the successful leaf.

Our predictions are then aligned with the corresponding target data, y, and merged into a single vector so average results can be computed.

A confusion matrix is formed using the built-in function "confusionmat".

Our recall rates, precision rates and the f1-measure are then computed for each emotion from the confusion matrix in the following way:

classification rate = |classified| / |predictions| * 100

recall rate (e) = confusion matrix(e,e) / sum over i {confusion matrix(e,i)} * 100
precision rate (e) = confusion matrix(e,e) / sum over j {confusion matrix(j, e)} * 100

f1(e) = 2 * precision rates(e) * recall rate(e) / (precision rates(e) + recall rate(e)) 
